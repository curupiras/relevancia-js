{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96177cd5",
   "metadata": {},
   "source": [
    "# Teste 3 - Criando embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42dba2",
   "metadata": {},
   "source": [
    "## 1. Cria dataframe com 4 conjuntos cada um com 3 frases distintas em inglês. Frases do mesmo conjunto devem ter uma forte relação semântica, enquanto frases em conjuntos diferentes são distantes semanticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a4b84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Conjunto  \\\n",
      "0        Technology and Innovation   \n",
      "1        Technology and Innovation   \n",
      "2        Technology and Innovation   \n",
      "3   Climate Change and Environment   \n",
      "4   Climate Change and Environment   \n",
      "5   Climate Change and Environment   \n",
      "6            Health and Well-being   \n",
      "7            Health and Well-being   \n",
      "8            Health and Well-being   \n",
      "9               Travel and Culture   \n",
      "10              Travel and Culture   \n",
      "11              Travel and Culture   \n",
      "\n",
      "                                                Frase  \n",
      "0   Advancements in artificial intelligence are tr...  \n",
      "1   The development of quantum computing holds the...  \n",
      "2   Innovative technologies like blockchain are re...  \n",
      "3   Global warming is leading to more extreme weat...  \n",
      "4   Deforestation contributes significantly to the...  \n",
      "5   Renewable energy sources are crucial for reduc...  \n",
      "6   Regular exercise is key to maintaining a healt...  \n",
      "7   Mental health awareness is becoming increasing...  \n",
      "8   Balanced nutrition is essential for physical a...  \n",
      "9   Exploring different cultures enriches our unde...  \n",
      "10  Travel restrictions have impacted internationa...  \n",
      "11  Learning a new language opens up opportunities...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dados das frases organizados em um dicionário com os nomes dos conjuntos em inglês\n",
    "data = {\n",
    "    \"Conjunto\": [\"Technology and Innovation\", \"Technology and Innovation\", \"Technology and Innovation\",\n",
    "                 \"Climate Change and Environment\", \"Climate Change and Environment\", \"Climate Change and Environment\",\n",
    "                 \"Health and Well-being\", \"Health and Well-being\", \"Health and Well-being\",\n",
    "                 \"Travel and Culture\", \"Travel and Culture\", \"Travel and Culture\"],\n",
    "    \"Frase\": [\"Advancements in artificial intelligence are transforming industries.\",\n",
    "              \"The development of quantum computing holds the potential to revolutionize data processing.\",\n",
    "              \"Innovative technologies like blockchain are reshaping financial transactions.\",\n",
    "              \"Global warming is leading to more extreme weather patterns.\",\n",
    "              \"Deforestation contributes significantly to the increase in atmospheric carbon dioxide levels.\",\n",
    "              \"Renewable energy sources are crucial for reducing greenhouse gas emissions.\",\n",
    "              \"Regular exercise is key to maintaining a healthy lifestyle.\",\n",
    "              \"Mental health awareness is becoming increasingly important in society.\",\n",
    "              \"Balanced nutrition is essential for physical and mental well-being.\",\n",
    "              \"Exploring different cultures enriches our understanding of the world.\",\n",
    "              \"Travel restrictions have impacted international tourism significantly.\",\n",
    "              \"Learning a new language opens up opportunities for cultural exchange.\"]\n",
    "}\n",
    "\n",
    "# Criando o DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Exibindo o DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d4d2e",
   "metadata": {},
   "source": [
    "## 2. Tokenização do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0801d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santosr\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#Carregando o nokenizador Distilbert\n",
    "#model_ckpt = \"distilbert-base-uncased\"\n",
    "model_ckpt = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951b8ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição da função que realizará a tokenização em lotes\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Frase\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b5c515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 12607, 2015, 1999, 7976, 4454, 2024, 17903, 6088, 1012, 102, 0, 0, 0, 0, 0], [101, 1996, 2458, 1997, 8559, 9798, 4324, 1996, 4022, 2000, 4329, 4697, 2951, 6364, 1012, 102], [101, 9525, 6786, 2066, 3796, 24925, 2078, 2024, 24501, 3270, 4691, 3361, 11817, 1012, 102, 0], [101, 3795, 12959, 2003, 2877, 2000, 2062, 6034, 4633, 7060, 1012, 102, 0, 0, 0, 0], [101, 13366, 25794, 16605, 6022, 2000, 1996, 3623, 1999, 12483, 6351, 14384, 3798, 1012, 102, 0], [101, 13918, 2943, 4216, 2024, 10232, 2005, 8161, 16635, 3806, 11768, 1012, 102, 0, 0, 0], [101, 3180, 6912, 2003, 3145, 2000, 8498, 1037, 7965, 9580, 1012, 102, 0, 0, 0, 0], [101, 5177, 2740, 7073, 2003, 3352, 6233, 2590, 1999, 2554, 1012, 102, 0, 0, 0, 0], [101, 12042, 14266, 2003, 6827, 2005, 3558, 1998, 5177, 2092, 1011, 2108, 1012, 102, 0, 0], [101, 11131, 2367, 8578, 4372, 13149, 2229, 2256, 4824, 1997, 1996, 2088, 1012, 102, 0, 0], [101, 3604, 9259, 2031, 19209, 2248, 6813, 6022, 1012, 102, 0, 0, 0, 0, 0, 0], [101, 4083, 1037, 2047, 2653, 7480, 2039, 6695, 2005, 3451, 3863, 1012, 102, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a função de tokenização aos dados\n",
    "data_encoded = data.copy()\n",
    "tokenized_outputs = tokenize(data_encoded)\n",
    "tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21a58bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Conjunto', 'Frase', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Armazenando input_ids e attention_mask em data_encoded\n",
    "data_encoded['input_ids'] = tokenized_outputs['input_ids']\n",
    "data_encoded['attention_mask'] = tokenized_outputs['attention_mask']\n",
    "data_encoded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90141543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: Advancements in artificial intelligence are transforming industries., \n",
      "Input IDs: ['[CLS]', 'advancement', '##s', 'in', 'artificial', 'intelligence', 'are', 'transforming', 'industries', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Frase: The development of quantum computing holds the potential to revolutionize data processing., \n",
      "Input IDs: ['[CLS]', 'the', 'development', 'of', 'quantum', 'computing', 'holds', 'the', 'potential', 'to', 'revolution', '##ize', 'data', 'processing', '.', '[SEP]']\n",
      "\n",
      "Frase: Innovative technologies like blockchain are reshaping financial transactions., \n",
      "Input IDs: ['[CLS]', 'innovative', 'technologies', 'like', 'block', '##chai', '##n', 'are', 'res', '##ha', '##ping', 'financial', 'transactions', '.', '[SEP]', '[PAD]']\n",
      "\n",
      "Frase: Global warming is leading to more extreme weather patterns., \n",
      "Input IDs: ['[CLS]', 'global', 'warming', 'is', 'leading', 'to', 'more', 'extreme', 'weather', 'patterns', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Frase: Deforestation contributes significantly to the increase in atmospheric carbon dioxide levels., \n",
      "Input IDs: ['[CLS]', 'def', '##orestation', 'contributes', 'significantly', 'to', 'the', 'increase', 'in', 'atmospheric', 'carbon', 'dioxide', 'levels', '.', '[SEP]', '[PAD]']\n",
      "\n",
      "Frase: Renewable energy sources are crucial for reducing greenhouse gas emissions., \n",
      "Input IDs: ['[CLS]', 'renewable', 'energy', 'sources', 'are', 'crucial', 'for', 'reducing', 'greenhouse', 'gas', 'emissions', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Frase: Regular exercise is key to maintaining a healthy lifestyle., \n",
      "Input IDs: ['[CLS]', 'regular', 'exercise', 'is', 'key', 'to', 'maintaining', 'a', 'healthy', 'lifestyle', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Frase: Mental health awareness is becoming increasingly important in society., \n",
      "Input IDs: ['[CLS]', 'mental', 'health', 'awareness', 'is', 'becoming', 'increasingly', 'important', 'in', 'society', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Frase: Balanced nutrition is essential for physical and mental well-being., \n",
      "Input IDs: ['[CLS]', 'balanced', 'nutrition', 'is', 'essential', 'for', 'physical', 'and', 'mental', 'well', '-', 'being', '.', '[SEP]', '[PAD]', '[PAD]']\n",
      "\n",
      "Frase: Exploring different cultures enriches our understanding of the world., \n",
      "Input IDs: ['[CLS]', 'exploring', 'different', 'cultures', 'en', '##rich', '##es', 'our', 'understanding', 'of', 'the', 'world', '.', '[SEP]', '[PAD]', '[PAD]']\n",
      "\n",
      "Frase: Travel restrictions have impacted international tourism significantly., \n",
      "Input IDs: ['[CLS]', 'travel', 'restrictions', 'have', 'impacted', 'international', 'tourism', 'significantly', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Frase: Learning a new language opens up opportunities for cultural exchange., \n",
      "Input IDs: ['[CLS]', 'learning', 'a', 'new', 'language', 'opens', 'up', 'opportunities', 'for', 'cultural', 'exchange', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for frase, input_id in zip(data_encoded[\"Frase\"], data_encoded[\"input_ids\"]):\n",
    "    print(f\"Frase: {frase}, \\nInput IDs: {tokenizer.convert_ids_to_tokens(input_id)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf804c",
   "metadata": {},
   "source": [
    "## 3. Obtenção dos embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c989cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "#Carregar modelo distilbert\n",
    "#model_ckpt = \"distilbert-base-uncased\"\n",
    "\n",
    "#Caso exista GPU utilize-a, caso contrário use a CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69222338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para extração da última camada oculta (apenas a representação do token [CLS])\n",
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "231e56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83666f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para extração da última camada oculta (apenas a representação do token [CLS])\n",
    "def extract_hidden_states_mean_pooling(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**inputs)\n",
    "    # \n",
    "    return {\"hidden_state\": mean_pooling(model_output, inputs['attention_mask']).cpu().numpy()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c2ceb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforma os input_ids e attention_mask em tensores\n",
    "data_encoded['input_ids'] = torch.tensor(data_encoded['input_ids'])\n",
    "data_encoded['attention_mask'] = torch.tensor(data_encoded['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8131178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrai a última camada oculta de data_encoded e armazena em hidden_state\n",
    "hidden_state = extract_hidden_states_mean_pooling(data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b228ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Conjunto': ['Technology and Innovation',\n",
       "  'Technology and Innovation',\n",
       "  'Technology and Innovation',\n",
       "  'Climate Change and Environment',\n",
       "  'Climate Change and Environment',\n",
       "  'Climate Change and Environment',\n",
       "  'Health and Well-being',\n",
       "  'Health and Well-being',\n",
       "  'Health and Well-being',\n",
       "  'Travel and Culture',\n",
       "  'Travel and Culture',\n",
       "  'Travel and Culture'],\n",
       " 'Frase': ['Advancements in artificial intelligence are transforming industries.',\n",
       "  'The development of quantum computing holds the potential to revolutionize data processing.',\n",
       "  'Innovative technologies like blockchain are reshaping financial transactions.',\n",
       "  'Global warming is leading to more extreme weather patterns.',\n",
       "  'Deforestation contributes significantly to the increase in atmospheric carbon dioxide levels.',\n",
       "  'Renewable energy sources are crucial for reducing greenhouse gas emissions.',\n",
       "  'Regular exercise is key to maintaining a healthy lifestyle.',\n",
       "  'Mental health awareness is becoming increasingly important in society.',\n",
       "  'Balanced nutrition is essential for physical and mental well-being.',\n",
       "  'Exploring different cultures enriches our understanding of the world.',\n",
       "  'Travel restrictions have impacted international tourism significantly.',\n",
       "  'Learning a new language opens up opportunities for cultural exchange.'],\n",
       " 'input_ids': tensor([[  101, 12607,  2015,  1999,  7976,  4454,  2024, 17903,  6088,  1012,\n",
       "            102,     0,     0,     0,     0,     0],\n",
       "         [  101,  1996,  2458,  1997,  8559,  9798,  4324,  1996,  4022,  2000,\n",
       "           4329,  4697,  2951,  6364,  1012,   102],\n",
       "         [  101,  9525,  6786,  2066,  3796, 24925,  2078,  2024, 24501,  3270,\n",
       "           4691,  3361, 11817,  1012,   102,     0],\n",
       "         [  101,  3795, 12959,  2003,  2877,  2000,  2062,  6034,  4633,  7060,\n",
       "           1012,   102,     0,     0,     0,     0],\n",
       "         [  101, 13366, 25794, 16605,  6022,  2000,  1996,  3623,  1999, 12483,\n",
       "           6351, 14384,  3798,  1012,   102,     0],\n",
       "         [  101, 13918,  2943,  4216,  2024, 10232,  2005,  8161, 16635,  3806,\n",
       "          11768,  1012,   102,     0,     0,     0],\n",
       "         [  101,  3180,  6912,  2003,  3145,  2000,  8498,  1037,  7965,  9580,\n",
       "           1012,   102,     0,     0,     0,     0],\n",
       "         [  101,  5177,  2740,  7073,  2003,  3352,  6233,  2590,  1999,  2554,\n",
       "           1012,   102,     0,     0,     0,     0],\n",
       "         [  101, 12042, 14266,  2003,  6827,  2005,  3558,  1998,  5177,  2092,\n",
       "           1011,  2108,  1012,   102,     0,     0],\n",
       "         [  101, 11131,  2367,  8578,  4372, 13149,  2229,  2256,  4824,  1997,\n",
       "           1996,  2088,  1012,   102,     0,     0],\n",
       "         [  101,  3604,  9259,  2031, 19209,  2248,  6813,  6022,  1012,   102,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  4083,  1037,  2047,  2653,  7480,  2039,  6695,  2005,  3451,\n",
       "           3863,  1012,   102,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]),\n",
       " 'hidden_state': tensor([[ 0.1392, -0.1941,  0.2469,  ..., -0.6203,  0.3433, -0.0394],\n",
       "         [-0.2990,  0.0208, -0.1526,  ..., -0.1567,  0.0116,  0.0822],\n",
       "         [-0.2190,  0.1069, -0.1734,  ..., -0.5992,  0.3779, -0.0114],\n",
       "         ...,\n",
       "         [ 0.2861,  0.3759, -0.0941,  ...,  0.0761, -0.1303, -0.2133],\n",
       "         [ 0.7307,  0.1830,  0.2148,  ..., -0.4883,  0.1045, -0.3246],\n",
       "         [ 0.2558, -0.1434,  0.1787,  ...,  0.3475,  0.0109, -0.0309]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transforma hidden_state em tensor\n",
    "data_hidden = data_encoded.copy()\n",
    "data_hidden['hidden_state'] = torch.tensor(hidden_state['hidden_state'])\n",
    "data_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9842bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 384])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensões de data_hidden\n",
    "data_hidden['hidden_state'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde4d03",
   "metadata": {},
   "source": [
    "## 4. Cálculo da distância entre os embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28814874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridade por cosseno: 0.36288851499557495\n"
     ]
    }
   ],
   "source": [
    "# Extraindo os embeddings de duas frases\n",
    "embedding1_tensor = data_hidden['hidden_state'][0]\n",
    "embedding2_tensor = data_hidden['hidden_state'][1]\n",
    "\n",
    "# Normalizando os embeddings\n",
    "embedding1_norm = embedding1_tensor / embedding1_tensor.norm()\n",
    "embedding2_norm = embedding2_tensor / embedding2_tensor.norm()\n",
    "\n",
    "# Calculando a similaridade por cosseno\n",
    "cosine_similarity = torch.dot(embedding1_norm, embedding2_norm)\n",
    "\n",
    "print(f\"Similaridade por cosseno: {cosine_similarity.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2b22b0",
   "metadata": {},
   "source": [
    "## 5. Estratégia com Sentence-Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d22fd90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridade por cosseno: 0.36288851499557495\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(model_ckpt)\n",
    "embeddings = model.encode(data['Frase'])\n",
    "\n",
    "# Calculando a similaridade por cosseno\n",
    "cosine_similarity = torch.dot(torch.tensor(embeddings[0]), torch.tensor(embeddings[1]))\n",
    "print(f\"Similaridade por cosseno: {cosine_similarity.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
