{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bdeb60",
   "metadata": {},
   "source": [
    "## 1. Carrega as bases de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf46976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai --trusted-host pypi.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8032a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Modelos\n",
    "MODELO = 'text-embedding-3-large'\n",
    "\n",
    "# Pasta com os dados\n",
    "PASTA_DADOS = './dados/'\n",
    "\n",
    "# Pasta com os dados de jurisprudência já tratados\n",
    "PASTA_JURIS_TCU = f'{PASTA_DADOS}outputs/1_tratamento_juris_tcu/'\n",
    "\n",
    "# Pasta onde serão armazenados os resultados desse caderno\n",
    "PASTA_RESULTADO_CADERNO = f'{PASTA_DADOS}outputs/6_gera_embeddings_js_open_ai/{MODELO}/'\n",
    "\n",
    "# Tamanho do lote\n",
    "#QUANTIDADE_DE_ITENS_CARREGADOS = 5\n",
    "SOBRESCREVER_EMBEDDINGS = False\n",
    "TAMANHO_DO_LOTE = 512\n",
    "\n",
    "# Função que carrega os arquivos \n",
    "def carrega_juris_tcu():\n",
    "    doc1 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_1.csv', sep='|')\n",
    "    doc2 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_2.csv', sep='|')\n",
    "    doc3 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_3.csv', sep='|')\n",
    "    doc4 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_4.csv', sep='|')\n",
    "    doc = pd.concat([doc1, doc2, doc3, doc4], ignore_index=True)\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc690a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from formatador import remove_html\n",
    "\n",
    "# Carrega os arquivos para doc\n",
    "doc = carrega_juris_tcu()\n",
    "#doc = doc.head(QUANTIDADE_DE_ITENS_CARREGADOS)\n",
    "\n",
    "#Remove tags do enunciado\n",
    "doc['ENUNCIADO'] = doc['ENUNCIADO'].apply(remove_html)\n",
    "\n",
    "#Transforma dataframe em dicionário\n",
    "doc = doc.to_dict(orient='list')\n",
    "print(doc.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc7648",
   "metadata": {},
   "source": [
    "## 2. Obtenção dos embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para dividir dicionário em lotes\n",
    "def dividir_dicionario_em_lotes(dicionario, tamanho_do_lote):\n",
    "    vetor_de_dicionarios = []\n",
    "    max_len = max(len(v) for v in dicionario.values())  # Encontrando o vetor de valores mais longo\n",
    "    \n",
    "    for i in range(0, max_len, tamanho_do_lote):\n",
    "        novo_dicionario = {chave: valores[i:i + tamanho_do_lote] for chave, valores in dicionario.items()}\n",
    "        vetor_de_dicionarios.append(novo_dicionario)\n",
    "    \n",
    "    return vetor_de_dicionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Função para reconstruir dicionário a partir de lotes\n",
    "def reconstruir_dicionario_a_partir_de_lotes(vetor_de_dicionarios):\n",
    "    dicionario_reconstruido = {}\n",
    "    \n",
    "    # Inicializando listas vazias para cada chave no primeiro dicionário do vetor\n",
    "    for chave in vetor_de_dicionarios[0].keys():\n",
    "        dicionario_reconstruido[chave] = []\n",
    "    \n",
    "    # Iterando sobre cada dicionário no vetor e concatenando os valores para cada chave\n",
    "    for dicionario_lote in vetor_de_dicionarios:\n",
    "        for chave, valores in dicionario_lote.items():\n",
    "            dicionario_reconstruido[chave].extend(valores)\n",
    "    \n",
    "    # Transforma numpy array em tensor\n",
    "    dicionario_reconstruido['mean_hidden_state'] = torch.tensor(np.array(dicionario_reconstruido['mean_hidden_state']))\n",
    "    \n",
    "    return dicionario_reconstruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d51625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para converter dicionário em dataframe\n",
    "def dicionario_para_dataframe(dicionario):\n",
    "    df = pd.DataFrame(dicionario)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para converter dataframe em dicionario\n",
    "def dataframe_para_dicionario(df):\n",
    "    return df.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass('Qual a chave da OpenAI?')\n",
    "\n",
    "# Função para extrair embeddings\n",
    "def get_embedding(texto, model=MODELO):\n",
    "    texto = texto.replace(\"\\n\", \" \")\n",
    "    return openai.embeddings.create(input = [texto], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processar e salvar embeddings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Divide doc_encoded em lotes\n",
    "doc_em_lotes = dividir_dicionario_em_lotes(doc, TAMANHO_DO_LOTE)\n",
    "\n",
    "# Processa e salva embeddings\n",
    "for i, dicionario in enumerate(tqdm(doc_em_lotes), start=1):\n",
    "    \n",
    "    caminho_arquivo = f'{PASTA_RESULTADO_CADERNO}{MODELO}_embeddings_js_{i}.pickle'\n",
    "    if  not SOBRESCREVER_EMBEDDINGS and os.path.exists(caminho_arquivo):\n",
    "        continue\n",
    "\n",
    "    dicionario_df = dicionario_para_dataframe(dicionario) \n",
    "    dicionario_df['mean_hidden_state'] = dicionario_df.ENUNCIADO.apply(lambda x: get_embedding(x, model=MODELO))\n",
    "    \n",
    "    # Cria estrutura que será salva em arquivo\n",
    "    embeddings_js = {\n",
    "        'key': dicionario_df['KEY'].tolist(),\n",
    "        'mean_hidden_state': dicionario_df['mean_hidden_state'].tolist()\n",
    "    }\n",
    "    \n",
    "    # Gravando lote em um arquivo .pickle\n",
    "    with open(caminho_arquivo, 'wb') as arquivo_pickle:\n",
    "        pickle.dump(embeddings_js, arquivo_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba5483",
   "metadata": {},
   "source": [
    "## 4. Cálculo da distância entre os embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para restaurar embeddings dos arquivos pickle\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def restaurar_doc_encoded_de_pickle(pasta_resultado_caderno):\n",
    "    # Lista para armazenar os dicionários lidos dos arquivos .pickle\n",
    "    doc_encoded_restaurado = []\n",
    "\n",
    "    # Listando todos os arquivos .pickle no diretório especificado\n",
    "    arquivos_pickle = [arq for arq in os.listdir(pasta_resultado_caderno) if arq.endswith('.pickle')]\n",
    "\n",
    "    # Ordenando os arquivos pelo número (assumindo que os nomes dos arquivos seguem o padrão embeddings_js_X.pickle)\n",
    "    arquivos_pickle.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "    # Lendo cada arquivo .pickle e restaurando o dicionário\n",
    "    for nome_arquivo in arquivos_pickle:\n",
    "        caminho_arquivo = os.path.join(pasta_resultado_caderno, nome_arquivo)\n",
    "        with open(caminho_arquivo, 'rb') as arquivo_pickle:\n",
    "            dicionario_restaurado = pickle.load(arquivo_pickle)\n",
    "            doc_encoded_restaurado.append(dicionario_restaurado)\n",
    "\n",
    "    return reconstruir_dicionario_a_partir_de_lotes(doc_encoded_restaurado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_encoded_restaurado = restaurar_doc_encoded_de_pickle(PASTA_RESULTADO_CADERNO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50cf3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc_hidden = doc.copy()\n",
    "doc_hidden['mean_hidden_state'] = doc_encoded_restaurado['mean_hidden_state']\n",
    "print(f\"mean_hidden_state: {doc_hidden['mean_hidden_state'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ac446",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo os embeddings de duas frases\n",
    "embedding1_tensor = doc_hidden['mean_hidden_state'][0]\n",
    "embedding2_tensor = doc_hidden['mean_hidden_state'][1]\n",
    "\n",
    "# Normalizando os embeddings\n",
    "embedding1_norm = embedding1_tensor / embedding1_tensor.norm()\n",
    "embedding2_norm = embedding2_tensor / embedding2_tensor.norm()\n",
    "\n",
    "# Calculando a similaridade por cosseno\n",
    "cosine_similarity = torch.dot(embedding1_norm, embedding2_norm)\n",
    "\n",
    "print(f\"Similaridade por cosseno: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2677dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_encoded_restaurqado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_hidden['ENUNCIADO'][0])\n",
    "print(doc_hidden['ENUNCIADO'][1])\n",
    "print(doc_hidden['ENUNCIADO'][4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
