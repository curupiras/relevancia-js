{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ebc4e17",
   "metadata": {},
   "source": [
    "Carregar dados da js em um dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac981207",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_local = load_dataset(\"csv\", data_files=\"train.txt\", sep=\";\", \n",
    "                              names=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685cf255",
   "metadata": {},
   "source": [
    "Transforma o Dataset em DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emotions.set_format(type=\"pandas\")\n",
    "df = emotions[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090626fd",
   "metadata": {},
   "source": [
    "Verificar quantas palavras existem nos enunciados\n",
    "Texts that\n",
    "are longer than a model’s context size need to be truncated, which can lead to a loss in\n",
    "performance if the truncated text contains crucial information; in this case, it looks\n",
    "like that won’t be an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\n",
    "df.boxplot(\"Words Per Tweet\", by=\"label_name\", grid=False, showfliers=False,\n",
    "           color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0eae1",
   "metadata": {},
   "source": [
    "Após exploração inicial dos dados resetar o formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895242d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb013ca",
   "metadata": {},
   "source": [
    "Transformer models like DistilBERT cannot receive raw strings as input; instead, they\n",
    "assume the text has been tokenized and encoded as numerical vectors. Tokenization is\n",
    "the step of breaking down a string into the atomic units used in the model. There are\n",
    "several tokenization strategies one can adopt, and the optimal splitting of words into\n",
    "subunits is usually learned from the corpus. Before looking at the tokenizer used for\n",
    "DistilBERT, let’s consider two extreme cases: character and word tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b211e",
   "metadata": {},
   "source": [
    "Tokenização por char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb3c51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481cd886",
   "metadata": {},
   "source": [
    "This is a good start, but we’re not done yet. Our model expects each character to be\n",
    "converted to an integer, a process sometimes called numericalization. One simple way\n",
    "to do this is by encoding each unique token (which are characters in this case) with a\n",
    "unique integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e8207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "141de440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a58a4e",
   "metadata": {},
   "source": [
    "The problem with this approach is that it creates a fictitious ordering between the names, and neural networks are really good at learning these kinds of relationships. So instead, we can create a new column for each category and assign a 1 where the category is true, and a 0 otherwise.\n",
    "\n",
    "On the other hand, the result of adding two one-hot encodings can easily be interpreted: the two entries that are \"hot\" indicate that the corresponding tokens co-occur. We can create the one-hot encodings in PyTorch by converting input_ids to a tensor and applying the one_hot() function as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2ee1d",
   "metadata": {},
   "source": [
    "From our simple example we can see that **character-level tokenization ignores any\n",
    "structure in the text and treats the whole string as a stream of characters**. Although\n",
    "this helps deal with misspellings and rare words, the main drawback is that **linguistic\n",
    "structures such as words need to be learned from the data**. This requires significant\n",
    "compute, memory, and data. For this reason, **character tokenization is rarely used in\n",
    "practice**. Instead, some structure of the text is preserved during the tokenization step.\n",
    "Word tokenization is a straightforward approach to achieve this, so let’s take a look at\n",
    "how it works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49558a1",
   "metadata": {},
   "source": [
    "Instead of splitting the text into characters, we can split it into words and map each\n",
    "word to an integer. Using words from the outset enables the model to skip the step of\n",
    "learning words from characters, and thereby reduces the complexity of the training\n",
    "process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c72a0e",
   "metadata": {},
   "source": [
    "One simple class of word tokenizers uses whitespace to tokenize the text. We can do this by applying Python's `split()` function directly on the raw text (just like we did to measure the tweet lengths):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3608ef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4e08b",
   "metadata": {},
   "source": [
    "From here we can take the same steps we took for the character tokenizer to map\n",
    "each word to an ID. However, we can already see one potential problem with this\n",
    "tokenization scheme: punctuation is not accounted for, so NLP. is treated as a single\n",
    "token. Given that words can include declinations, conjugations, or misspellings, the\n",
    "size of the vocabulary can easily grow into the millions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e66304",
   "metadata": {},
   "source": [
    "Having a large vocabulary is a problem because it requires neural networks to have an\n",
    "enormous number of parameters.\n",
    "\n",
    "Naturally, we want to avoid being so wasteful with our model parameters since mod‐\n",
    "els are expensive to train, and larger models are more difficult to maintain. A com‐\n",
    "mon approach is to limit the vocabulary and discard rare words by considering, say,\n",
    "the 100,000 most common words in the corpus. Words that are not part of the\n",
    "vocabulary are classified as “unknown” and mapped to a shared UNK token. This\n",
    "means that we lose some potentially important information in the process of word\n",
    "tokenization, since the model has no information about words associated with UNK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b120cb7a",
   "metadata": {},
   "source": [
    "Wouldn’t it be nice if there was a compromise between character and word tokeniza‐\n",
    "tion that preserved all the input information and some of the input structure? There\n",
    "is: subword tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd51b27",
   "metadata": {},
   "source": [
    "On the one hand, we want to split rare words into smaller\n",
    "units to allow the model to deal with complex words and misspellings. On the other\n",
    "hand, we want to keep frequent words as unique entities so that we can keep the\n",
    "length of our inputs to a manageable size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8802a5",
   "metadata": {},
   "source": [
    "When using pretrained models, it is really important to make sure\n",
    "that you use the same tokenizer that the model was trained with.\n",
    "From the model’s perspective, switching the tokenizer is like shuf‐\n",
    "fling the vocabulary. If everyone around you started swapping\n",
    "random words like “house” for “cat,” you’d have a hard time under‐\n",
    "standing what was going on too!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
