{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bdeb60",
   "metadata": {},
   "source": [
    "## 1. Carrega as bases de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe8032a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "###############################################################################\n",
    "# Modelos\n",
    "#MODELO = 'rufimelo/Legal-BERTimbau-sts-large-ma-v3'\n",
    "#MAX_SEQ_LENGTH = 512\n",
    "\n",
    "#MODELO = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "#MAX_SEQ_LENGTH = 128\n",
    "\n",
    "#MODELO = 'neuralmind/bert-large-portuguese-cased'\n",
    "#MAX_SEQ_LENGTH = 512\n",
    "\n",
    "#MODELO = 'stjiris/bert-large-portuguese-cased-legal-mlm-sts-v1.0'\n",
    "#MAX_SEQ_LENGTH = 512\n",
    "\n",
    "MODELO = 'stjiris/bert-large-portuguese-cased-legal-mlm-nli-sts-v1'\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "#MODELO = 'Luciano/bert-base-portuguese-cased-finetuned-tcu-acordaos'\n",
    "#MAX_SEQ_LENGTH = 512\n",
    "\n",
    "#MODELO = 'neuralmind/bert-base-portuguese-cased'\n",
    "#MAX_SEQ_LENGTH = 512\n",
    "###############################################################################\n",
    "\n",
    "CAMINHO_MODELO = MODELO.split(\"/\")[-1]\n",
    "\n",
    "PASTA_DADOS = './dados/'\n",
    "\n",
    "# A pasta dos JURIS aqui não é a pasta original, e sim o resultado do caderno 1\n",
    "PASTA_JURIS_TCU = f'{PASTA_DADOS}outputs/1_tratamento_juris_tcu/'\n",
    "\n",
    "PASTA_RESULTADO_CADERNO = f'{PASTA_DADOS}outputs/7_gera_embeddings_termos/{CAMINHO_MODELO}/'\n",
    "\n",
    "# Substituir embeddings já criados\n",
    "SOBRESCREVER_EMBEDDINGS = False\n",
    "\n",
    "# Tamanho do lote\n",
    "TAMANHO_DO_LOTE = 1\n",
    "\n",
    "# Carrega os arquivos \n",
    "def carrega_juris_tcu():\n",
    "    doc1 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_1.csv', sep='|')\n",
    "    doc2 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_2.csv', sep='|')\n",
    "    doc3 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_3.csv', sep='|')\n",
    "    doc4 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_4.csv', sep='|')\n",
    "    doc = pd.concat([doc1, doc2, doc3, doc4], ignore_index=True)\n",
    "    query = pd.read_csv(f'{PASTA_JURIS_TCU}query_tratado.csv', sep='|')\n",
    "    qrel = pd.read_csv(f'{PASTA_JURIS_TCU}qrel_tratado.csv', sep='|')\n",
    "\n",
    "    return doc, query, qrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e86d073-04ff-412a-81b2-e357538dc1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dados/outputs/7_gera_embeddings_termos/bert-large-portuguese-cased-legal-mlm-nli-sts-v1/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PASTA_RESULTADO_CADERNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc690a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['KEY', 'TEXT', 'SOURCE'])\n"
     ]
    }
   ],
   "source": [
    "from formatador import remove_html\n",
    "\n",
    "# Carrega as queries para query\n",
    "doc, query, qrel = carrega_juris_tcu()\n",
    "\n",
    "#Transforma dataframe em dicionário\n",
    "query = query.to_dict(orient='list')\n",
    "print(query.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3f683",
   "metadata": {},
   "source": [
    "## 2. Tokenização das queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7ba267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#Carregando o nokenizador Legal BERTimbau V3\n",
    "model_ckpt = MODELO\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d9e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição da função que realizará a tokenização em lotes\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"TEXT\"], padding=True, truncation=True, return_tensors='pt', max_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2557816d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  4204,   122,  ...,     0,     0,     0],\n",
       "        [  101,  8197,   123,  ...,     0,     0,     0],\n",
       "        [  101,   602, 16166,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,   807,   441,  ...,     0,     0,     0],\n",
       "        [  101, 17911, 22287,  ...,     0,     0,     0],\n",
       "        [  101, 13082,   253,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a função de tokenização aos dados\n",
    "query_encoded = query.copy()\n",
    "tokenized_outputs = tokenize(query_encoded)\n",
    "tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6612d1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['KEY', 'TEXT', 'SOURCE', 'input_ids', 'attention_mask', 'token_type_ids'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Armazenando input_ids, attention_mask e token_type_ids em query_encoded\n",
    "query_encoded['input_ids'] = tokenized_outputs['input_ids']\n",
    "query_encoded['attention_mask'] = tokenized_outputs['attention_mask']\n",
    "query_encoded['token_type_ids'] = tokenized_outputs['token_type_ids']\n",
    "query_encoded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7577cc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termo: técnica e preço, \n",
      "Input IDs: ['[CLS]', 'técnica', 'e', 'preço', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Termo: restos a pagar, \n",
      "Input IDs: ['[CLS]', 'restos', 'a', 'pagar', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Termo: aditivo a contrato, \n",
      "Input IDs: ['[CLS]', 'ad', '##itivo', 'a', 'contrato', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifica se a tokenização foi realizada adequadamente\n",
    "from itertools import islice\n",
    "\n",
    "for termo, input_id in islice(zip(query_encoded[\"TEXT\"], query_encoded[\"input_ids\"]), 3):\n",
    "    print(f\"Termo: {termo}, \\nInput IDs: {tokenizer.convert_ids_to_tokens(input_id)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc7648",
   "metadata": {},
   "source": [
    "## 3. Obtenção dos embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb3e4f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4733385f96cc4296a34cf198dc45fb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  34%|###4      | 461M/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santosr\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\santosr\\.cache\\huggingface\\hub\\models--stjiris--bert-large-portuguese-cased-legal-mlm-nli-sts-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Carrega modelo\n",
    "\n",
    "#Caso exista GPU utilize-a, caso contrário use a CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8158a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para agregaçção da última camada oculta pela média\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f33caf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extração da última camada oculta\n",
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**inputs)\n",
    "    \n",
    "    return batch['KEY'], model_output.last_hidden_state[:,0].cpu().numpy(), mean_pooling(model_output, inputs['attention_mask']).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e14d6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para dividir dicionário em lotes\n",
    "def dividir_dicionario_em_lotes(dicionario, tamanho_do_lote):\n",
    "    vetor_de_dicionarios = []\n",
    "    max_len = max(len(v) for v in dicionario.values())  # Encontrando o vetor de valores mais longo\n",
    "    \n",
    "    for i in range(0, max_len, tamanho_do_lote):\n",
    "        novo_dicionario = {chave: valores[i:i + tamanho_do_lote] for chave, valores in dicionario.items()}\n",
    "        vetor_de_dicionarios.append(novo_dicionario)\n",
    "    \n",
    "    return vetor_de_dicionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50bb1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Função para reconstruir dicionário a partir de lotes\n",
    "def reconstruir_dicionario_a_partir_de_lotes(vetor_de_dicionarios):\n",
    "    dicionario_reconstruido = {}\n",
    "    \n",
    "    # Inicializando listas vazias para cada chave no primeiro dicionário do vetor\n",
    "    for chave in vetor_de_dicionarios[0].keys():\n",
    "        dicionario_reconstruido[chave] = []\n",
    "    \n",
    "    # Iterando sobre cada dicionário no vetor e concatenando os valores para cada chave\n",
    "    for dicionario_lote in vetor_de_dicionarios:\n",
    "        for chave, valores in dicionario_lote.items():\n",
    "            dicionario_reconstruido[chave].extend(valores)\n",
    "    \n",
    "    # Transforma numpy array em tensor\n",
    "    dicionario_reconstruido['cls_hidden_state'] = torch.tensor(np.array(dicionario_reconstruido['cls_hidden_state']))\n",
    "    dicionario_reconstruido['mean_hidden_state'] = torch.tensor(np.array(dicionario_reconstruido['mean_hidden_state']))\n",
    "    \n",
    "    return dicionario_reconstruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "651d9667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:14<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Processar e salvar embeddings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Divide query_encoded em lotes\n",
    "query_encoded_em_lotes = dividir_dicionario_em_lotes(query_encoded, TAMANHO_DO_LOTE)\n",
    "\n",
    "# Processa e salva embeddings\n",
    "for i, dicionario in enumerate(tqdm(query_encoded_em_lotes), start=1):\n",
    "    \n",
    "    caminho_arquivo = f'{PASTA_RESULTADO_CADERNO}{CAMINHO_MODELO}_embeddings_query_{i}.pickle'\n",
    "    if  not SOBRESCREVER_EMBEDDINGS and os.path.exists(caminho_arquivo):\n",
    "        continue\n",
    "\n",
    "    key, cls_hidden_state, mean_hidden_state = extract_hidden_states(dicionario)\n",
    "    \n",
    "    # Cria estrutura que será salva em arquivo\n",
    "    embeddings_query = {\n",
    "        'key': key,\n",
    "        'cls_hidden_state': cls_hidden_state,\n",
    "        'mean_hidden_state': mean_hidden_state\n",
    "    }\n",
    "    \n",
    "    # Gravando lote em um arquivo .pickle\n",
    "    with open(caminho_arquivo, 'wb') as arquivo_pickle:\n",
    "        pickle.dump(embeddings_query, arquivo_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b395041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 46])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicionario['input_ids'].size()\n",
    "#dicionario['attention_mask']\n",
    "#dicionario['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa7dec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_encoded_em_lotes[0]['KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba5483",
   "metadata": {},
   "source": [
    "## 4. Cálculo da distância entre os embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3f2edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para restaurar embeddings dos arquivos pickle\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def restaurar_query_encoded_de_pickle(pasta_resultado_caderno):\n",
    "    # Lista para armazenar os dicionários lidos dos arquivos .pickle\n",
    "    query_encoded_restaurado = []\n",
    "\n",
    "    # Listando todos os arquivos .pickle no diretório especificado\n",
    "    arquivos_pickle = [arq for arq in os.listdir(pasta_resultado_caderno) if arq.endswith('.pickle')]\n",
    "\n",
    "    # Ordenando os arquivos pelo número (assumindo que os nomes dos arquivos seguem o padrão embeddings_query_X.pickle)\n",
    "    arquivos_pickle.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "    # Lendo cada arquivo .pickle e restaurando o dicionário\n",
    "    for nome_arquivo in arquivos_pickle:\n",
    "        caminho_arquivo = os.path.join(pasta_resultado_caderno, nome_arquivo)\n",
    "        with open(caminho_arquivo, 'rb') as arquivo_pickle:\n",
    "            dicionario_restaurado = pickle.load(arquivo_pickle)\n",
    "            query_encoded_restaurado.append(dicionario_restaurado)\n",
    "\n",
    "    return reconstruir_dicionario_a_partir_de_lotes(query_encoded_restaurado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38ca99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoded_restaurado = restaurar_query_encoded_de_pickle(PASTA_RESULTADO_CADERNO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e50cf3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_hidden_state: torch.Size([299, 1024])\n",
      "\n",
      "mean_hidden_state: torch.Size([299, 1024])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "query_hidden = query_encoded.copy()\n",
    "query_hidden['cls_hidden_state'] = query_encoded_restaurado['cls_hidden_state']\n",
    "query_hidden['mean_hidden_state'] = query_encoded_restaurado['mean_hidden_state']\n",
    "print(f\"cls_hidden_state: {query_hidden['cls_hidden_state'].size()}\\n\")\n",
    "print(f\"mean_hidden_state: {query_hidden['mean_hidden_state'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83f2af00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: técnica e preço\n",
      "Query 2: sobrepreço e superfaturamento\n",
      "Similaridade por cosseno: 0.504503607749939\n"
     ]
    }
   ],
   "source": [
    "# Extraindo os embeddings de duas queries\n",
    "embedding1_tensor = query_hidden['mean_hidden_state'][0]\n",
    "embedding2_tensor = query_hidden['mean_hidden_state'][4]\n",
    "\n",
    "# Normalizando os embeddings\n",
    "embedding1_norm = embedding1_tensor / embedding1_tensor.norm()\n",
    "embedding2_norm = embedding2_tensor / embedding2_tensor.norm()\n",
    "\n",
    "# Calculando a similaridade por cosseno\n",
    "cosine_similarity = torch.dot(embedding1_norm, embedding2_norm)\n",
    "\n",
    "print(f\"Query 1: {query_hidden['TEXT'][0]}\")\n",
    "print(f\"Query 2: {query_hidden['TEXT'][4]}\")\n",
    "print(f\"Similaridade por cosseno: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2677dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': [1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150],\n",
       " 'cls_hidden_state': tensor([[ 0.3179, -0.6954,  0.6730,  ...,  0.5940,  0.0730,  0.0502],\n",
       "         [-1.3415,  0.8941,  0.2905,  ...,  1.0589, -1.4596,  2.2038],\n",
       "         [ 0.4483, -0.5526, -0.0622,  ..., -0.3044, -1.2603, -0.1306],\n",
       "         ...,\n",
       "         [ 0.2185, -0.4574,  0.0821,  ...,  0.1981, -0.1217, -0.8987],\n",
       "         [-0.3483, -0.2588, -0.5191,  ..., -0.1863, -1.1302, -0.5748],\n",
       "         [ 0.1250, -0.1749,  0.1683,  ...,  0.6643, -1.2865,  0.3972]]),\n",
       " 'mean_hidden_state': tensor([[-0.2586, -1.0703,  0.8252,  ...,  0.6663, -0.0916, -0.0557],\n",
       "         [-1.3046,  0.6129,  0.3390,  ...,  1.1221, -1.2975,  1.8503],\n",
       "         [ 0.3827, -0.4580, -0.1296,  ..., -0.1017, -2.3925, -0.1220],\n",
       "         ...,\n",
       "         [-0.2627, -1.0084,  0.6357,  ...,  0.2449, -0.7193, -1.1067],\n",
       "         [-0.3509, -0.6514,  0.0835,  ..., -0.1525, -1.7563, -0.9559],\n",
       "         [-0.7513, -0.4261,  0.0907,  ...,  0.5139, -0.1220, -0.1602]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_encoded_restaurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32d4a746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "técnica e preço\n",
      "restos a pagar\n",
      "sobrepreço e superfaturamento\n"
     ]
    }
   ],
   "source": [
    "print(query_hidden['TEXT'][0])\n",
    "print(query_hidden['TEXT'][1])\n",
    "print(query_hidden['TEXT'][4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
